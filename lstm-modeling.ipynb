{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "LSTM Model Selection\n",
    "=============\n",
    "\n",
    "\n",
    "\n",
    "This is an implementation of training an LSTM character model which based on Udacity Deep Learning Course assignment source code. https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/udacity\n",
    "\n",
    "I used this for model hyper parameter selection, the real data train was performed on lua trainer.\n",
    "\n",
    "The major modification:\n",
    "------------\n",
    "1. added UTF-8 character support so that it can be used for chinese and other encoded corpus\n",
    "2. adopted embedding to avoid the sparsity of one-hot\n",
    "3. changed single layer lstm to multi-layer lstm\n",
    "4. added dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle files found, try to load data from pickle files...\n",
      "Data loaded from pickle files successfully\n",
      "Most common words (+UNK):\n",
      "\t UNK \t 1259386\n",
      "\t ， \t 5625618\n",
      "\t 的 \t 3907686\n",
      "\t 。 \t 3336048\n",
      "\t 1 \t 2079419\n",
      "\t 0 \t 1827941\n",
      "\t 年 \t 1607422\n",
      "\t 2 \t 1303876\n",
      "\t 、 \t 1282589\n",
      "\t 在 \t 1235414\n",
      "Least common words:\n",
      "\t 鳃 \t 1284\n",
      "\t 娄 \t 1282\n",
      "\t 骆 \t 1279\n",
      "\t 舅 \t 1279\n",
      "\t 敷 \t 1279\n",
      "\t 迭 \t 1276\n",
      "\t 磺 \t 1274\n",
      "\t 汐 \t 1271\n",
      "\t 砍 \t 1271\n",
      "\t 嵩 \t 1271\n",
      "Sample data [178, 37, 230, 1225, 178, 37, 2, 507, 900, 56]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 3000\n",
    "\n",
    "def build_dataset(words):\n",
    "  count = [['UNK', -1]]\n",
    "  count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "  dictionary = dict()\n",
    "  for word, _ in count:\n",
    "    dictionary[word] = len(dictionary)\n",
    "  data = list()\n",
    "  unk_count = 0\n",
    "  for word in words:\n",
    "    if word in dictionary:\n",
    "      index = dictionary[word]\n",
    "    else:\n",
    "      index = 0  # dictionary['UNK']\n",
    "      unk_count = unk_count + 1\n",
    "    data.append(index)\n",
    "  count[0][1] = unk_count\n",
    "  reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "def maybe_pickle(target_data, set_filename, force=False):\n",
    "  if os.path.exists(set_filename) and not force:\n",
    "    if os.path.getsize(set_filename) > 0:\n",
    "      # You may override by setting force=True.\n",
    "      print('%s already present - Skipping pickling.' % set_filename)\n",
    "      return set_filename\n",
    "  print('Pickling %s.' % set_filename)\n",
    "  try:\n",
    "    with open(set_filename, 'wb') as f:\n",
    "      pickle.dump(target_data, f, pickle.HIGHEST_PROTOCOL)\n",
    "  except Exception as e:\n",
    "    print('Unable to save data to', set_filename, ':', e)\n",
    "\n",
    "#with open(\"wiki_cn_chunk.txt\", 'r') as f:\n",
    "def loadData(data_file=\"data.pickle\", count_file=\"count.pickle\", dict_file=\"dictionary.pickle\", rev_dict_file=\"reverse_dictionary.pickle\", force=False):\n",
    "  if os.path.exists(data_file) and os.path.exists(count_file) and os.path.exists(dict_file) and os.path.exists(rev_dict_file) and not force:\n",
    "    try:\n",
    "      print(\"Pickle files found, try to load data from pickle files...\")\n",
    "      with open(data_file, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "      with open(count_file, 'rb') as f:\n",
    "        count = pickle.load(f)\n",
    "      with open(dict_file, 'rb') as f:\n",
    "        dictionary = pickle.load(f)\n",
    "      with open(rev_dict_file, 'rb') as f:\n",
    "        reverse_dictionary = pickle.load(f)\n",
    "      print(\"Data loaded from pickle files successfully\")\n",
    "      return data, count, dictionary, reverse_dictionary\n",
    "    except Exception as e:\n",
    "      print('Unable to load data', ':', e)\n",
    "  with open(\"../notebooks/wiki_cn\", 'r') as f:\n",
    "    print(\"Loading words from text file...\")\n",
    "    #lines = tf.compat.as_str(f.read().decode(\"utf-8\")).strip().split()\n",
    "    lines = f.read().strip().decode(\"utf-8\", \"ignore\").split()\n",
    "    #print(lines[:10])\n",
    "    words = []\n",
    "    for line in lines:\n",
    "        words.extend(list(line))\n",
    "    print('Data size %d' % len(words))\n",
    "    print(words[:10])\n",
    "    \n",
    "    print(\"Cooking data from words loaded...\")\n",
    "    data, count, dictionary, reverse_dictionary = build_dataset(words)\n",
    "    del words  # Hint to reduce memory.\n",
    "    \n",
    "    print(\"Saving cooked data into pickle files...\")\n",
    "    maybe_pickle(dictionary, \"dictionary.pickle\")\n",
    "    maybe_pickle(reverse_dictionary, \"reverse_dictionary.pickle\")\n",
    "    maybe_pickle(count, \"count.pickle\")\n",
    "    maybe_pickle(data, \"data.pickle\")\n",
    "  return data, count, dictionary, reverse_dictionary\n",
    "\n",
    "data, count, dictionary, reverse_dictionary = loadData()\n",
    "print('Most common words (+UNK):')\n",
    "for (wd, cnt) in count[:10]:\n",
    "  print(\"\\t\",wd,\"\\t\",cnt)\n",
    "print('Least common words:')\n",
    "for (wd, cnt) in count[-10:]:\n",
    "  print(\"\\t\",wd,\"\\t\",cnt)\n",
    "print('Sample data', data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 128)\n"
     ]
    }
   ],
   "source": [
    "with open(\"final_embeddings.pickle\", 'rb') as f:\n",
    "  embeddings = pickle.load(f)\n",
    "  \n",
    "print(embeddings.shape)\n",
    "\n",
    "vocabulary_size = embeddings.shape[0]\n",
    "embedding_size = embeddings.shape[1] # Dimension of the embedding vector.\n",
    "\n",
    "#embeddings = np.random.rand(vocabulary_size, embedding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160142993 [178, 37, 352, 31, 59, 45, 370, 2, 366, 605, 18, 271, 1, 479, 67, 82, 433, 1531, 148, 366, 605, 2, 349, 89, 570, 523, 3, 164, 10, 724, 76, 10, 366, 605, 59, 45, 2, 178, 37, 9, 69, 54, 503, 89, 366, 605, 159, 121, 205, 68, 18, 67, 1, 479, 45, 11, 178, 37, 1173, 740, 2, 10, 724, 507]\n",
      "1000 [178, 37, 230, 1225, 178, 37, 2, 507, 900, 56, 250, 67, 247, 13, 41, 23, 56, 336, 165, 59, 435, 14, 52, 91, 249, 1180, 2, 10, 1256, 3, 69, 230, 81, 1173, 740, 2, 772, 2085, 547, 9, 369, 854, 72, 8, 142, 836, 52, 241, 258, 155, 72, 369, 604, 189, 159, 2, 369, 114, 178, 37, 105, 81, 159, 661]\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "#valid_text = text[:valid_size]\n",
    "#train_text = text[valid_size:]\n",
    "#train_size = len(train_text)\n",
    "#print(train_size, train_text[:64])\n",
    "#print(valid_size, valid_text[:64])\n",
    "\n",
    "valid_text = data[:valid_size]\n",
    "train_text = data[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character:  \n",
      "Unexpected character: ï\n",
      "char2id:  17 773 0 0 1555\n",
      "Unexpected id: 3000\n",
      "id2char:  ， r UNK 亚 菌  \n"
     ]
    }
   ],
   "source": [
    "def char2id(char):\n",
    "  try:\n",
    "    return dictionary[char]\n",
    "  except Exception as e:\n",
    "    #print(e)\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "\n",
    "def id2char(dictid):\n",
    "  if dictid >= 0 and dictid < vocabulary_size:\n",
    "    return reverse_dictionary[dictid]\n",
    "  else:\n",
    "    print('Unexpected id: %d' % dictid)\n",
    "    return ' '\n",
    "\n",
    "print(\"char2id: \", char2id(u'a'), char2id(u'z'), char2id(u' '), char2id(u'ï'), char2id(u'菌'))\n",
    "print(\"id2char: \", id2char(1), id2char(26), id2char(0), id2char(155), id2char(1555), id2char(3000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "数\"科学则是在此之后的事。若认为/ 据统，路线总长22.5公里。革新/ 为艾萨克·牛顿爵士在他的花园里散/ 的西晋世族政治打下基础，形成“上/ 在500美元）。随后，在1971/ 之，同时存在著不同的汉语，比如说/ 对这种与西方世界以基督教治理殖民/ 明政府曾组织专家学者从文化、历史/ 对550年被封为奥尔良公爵。15/ 释铜佛洞”。灵光寺历经八国联军炮/ 少支持。以保守而知名的记者彼得·/ 中la_46。所以，第1至\"n\"/ 同向量formula_3为一个白/ 不列出一些选用字的例子，当中被废/ ））、维度卡（MarkViduk/ 失佛罗伦萨军事上的软弱。美第奇家/ 密腹面、五脏、血、经络(腹面、四/ 纪ydna）之后，罗马对马其顿的/ 部在原著小说中的X世代，很明显的/ 出ttp://www.china/ 亥后，光复会多数成员加入同盟会，/ 。西班牙语的辛德路斯结成朋友，而/ 关认为国民政府此项决议过于草率且/ 与手放到萤幕上，因为只有触控笔才/ 1，对比度为350:1，视角可确/ 次会发展成再生不良性贫血。一份中/ 器其特点是空心电枢轴通过齿形联结/ 理。本表仅仅提供较广泛笼统的说明/ 哈次两帝共治。马克奥里略人称“哲/ 862年），戴潮春命将攻陷大甲，/ 行0日，1973年10月南琪出版/ o而发生争执。此时Willy叫他/ 如阿尔弗雷德军事管理制度的复杂和/ 站着把它转变为已免费并开放原始码/ 纪时代是一段国际性与世界性的历史/ 共潮，资本主义工商业实现了全行业/ 为宋改砖塔，今石塔为南宋绍定元年/ 日05年欧洲冠军联赛决赛（于阿塔/ ，复数平面上仍然是一个单位圆，但/ ，部位可以与含丝氨酸的凝血酶和凝/ prpDevelop到Linux/ 就灾直升机失事事故。生平.邱光华/ 赛珠海国际赛车场进行学习与训练，/ 罗恩佐用这种方式赢得了很多赞助，/ 交小学。学校创立于于西元1900/ C兵、骑兵和战车。2010年冬季/ t”其所有元素的方法来表示这个它/ 海（4月2日免）、陈光甫（4月2/ e埔寨、缅甸、印度、不丹、尼泊尔/ 乌Islands）以东巡航；第二/ 专段时间考察以及介绍人的介绍后，/ t发生超新星爆炸的机率估计是10/ o，他还把曼森介绍给他娱乐圈的有/ 河陆上仅有的沙漠。多恩人的好斗性/ 计怒的埃塞俄比亚国王出兵占据该苏/ 东化钾溶液时，只能得到组成为K4/ K岸有古琴台，俞伯牙和钟子期高山/ 上。想见你(K的单曲)想见你（会/ 巧接近于1，随着两者间距增加可见/ 实见家纯、木村由信遭到杀害后投降/ 位报数。每一鞭都要在报完数后执行/ ，英外围赛则顺利取得首名参赛在乌/ 1进出口部总经理、经贸局总经理助/ 会区档案局西藏自治区档案局是中国/ \n",
      "为学并行的学科。1874年，日本/ 新亚特区第一任民选市长。1976/ 散（1859）、《克洛德的妻子》/ 上外有贸易往来。此外造船业、陶瓷/ 1型，也就是今日广受人知的福斯金/ 说着甲午战争的结束，条约现典藏于/ 民又以描写原住民少女帮助日人的《/ 史物质“异电性”。两个同电性物质/ 5共产党，是中共第十七届中央候补/ 炮。塔院最高处为地藏殿。塔院及周/ ·也是基石派（Cornersto/ \"行政首长萨鲁斯特以年事为由拒绝/ 白农民组合与台湾文化协会的全台巡/ 废有7名学生被捕后枪决，此事件后/ k宣布球场获冠名赞助称为“spo/ 家00瑞士雇佣兵与3000加斯科/ 四为目的设计的程序（例如，Lig/ 的亲密或尊敬程度而定。在马尔凯地/ 的室里面的壁画，上面述说著关于月/ a、霍亨索伦为邻。巴登面积约15/ ，0月11日），原名Benede/ 而5位，先后败予富咸、侯城、史笃/ 且。交通路线列表边贡边贡，字廷实/ 才并且能够迅速的作为无线X－Wi/ 确几乎就与本线平行建构。台六线旧/ 中ormula_22成立，这里f/ 结防空转防滑行装置则借鉴了8K型/ 明员，并在香港演艺学院担任教职。/ 哲勒良和加里恩努斯不得不率领两支/ ，子7.5公里10公里.子条目：/ 版多欧洲的城市都有天文钟。你可以/ 他江县训导时，阅览袁枢《通鉴纪事/ 和队。在云佩斯受伤之前，米杜士堡/ 码床生化学、血液学、血清学、免疫/ 史是希腊化时代最为优秀的医生。赫/ 业企业名称的一部分，附属于企业，/ 年统，但每次只能开启一个，很适合/ 塔，是他的最出色表现之一。5月2/ 但翼鸟夫妻来喻其同止同栖、双宿双/ 凝agdalaandtheoth/ x教育研究所取得硕士学位，197/ 华提出没有学生受伤，人性本能及社/ ，鬼来残害生灵。猴子屈服于她的意/ ，要的一步是雇用了乔治·莫伊尼汉/ 0与地球联合开战后，双方为求战胜/ 季蜥下目是由MaxFürbrin/ 它德国政府接收了46架前东德的S/ 2过，正因为其跨平台的特色，使教/ 尔，包括台湾海峡、东海、黄海、渤/ 二明码向海尔赛求救；但海尔赛未有/ ，系数在概率论的许多分支中都有应/ 000周年，该建筑于2000年在/ 有即将带来的天启做准备，绕着营火/ 性中老人”，一个乌龟神，战胜了蟹/ 苏游戏中的初期武器，一代是Glo/ 4年8月15日《生活知识报》，目/ 山正在恋爱中”是AKB48Tea/ 会t拥有官方博客、服务器状态查询/ 见是他造的。众神饮苏摩汁的杯子也/ 降王过世，为解决释迦族与拘利族因/ 行女犯，管理人员只可鞭打手掌心而/ 乌驻广州办事处为正局级单位，其他/ 助黎南面战线，诱法王迎战并予以牵/ 国拉萨市宇拓路1号，是西藏规模最/ \n",
      "数学/ \n",
      "学基/ "
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=15\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      candidate = self._text[self._cursor[b]]\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "      if(candidate == 0):\n",
    "        raise ValueError('UNK in context')\n",
    "      batch[b, candidate] = 1.0\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    noUNK = 0\n",
    "    while(not noUNK):\n",
    "      noUNK = 1\n",
    "      try:\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "          batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "      except ValueError:\n",
    "        noUNK = 0\n",
    "    return batches\n",
    "  \n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "#print(train_batches.next())\n",
    "#print(batches2string(train_batches.next()))\n",
    "print()\n",
    "for chstr in batches2string(train_batches.next()):\n",
    "  print(chstr+\"/ \", end='')\n",
    "print()\n",
    "for chstr in batches2string(train_batches.next()):\n",
    "  print(chstr+\"/ \", end='')\n",
    "\n",
    "#print(batches2string(valid_batches.next()))\n",
    "print()\n",
    "for chstr in batches2string(valid_batches.next()):\n",
    "  print(chstr+\"/ \", end='')\n",
    "print()\n",
    "for chstr in batches2string(valid_batches.next()):\n",
    "  print(chstr+\"/ \", end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "#num_nodes = 64\n",
    "num_nodes = 256\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  embd32 = tf.constant(embeddings, dtype=tf.float32)\n",
    "  embd64 = tf.constant(embeddings, dtype=tf.float64)\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([embedding_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state, keep_prob = 1.0):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    output = output_gate * tf.tanh(state)\n",
    "    output = tf.nn.dropout(output, keep_prob)\n",
    "    return output, state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    ##[TO-DO:] covert input (1hot) into embeddings before lstm cell\n",
    "    ##         (id -> one hot -> embedding)  tf.constant(embeddings)\n",
    "    i = tf.matmul(i, embd32)\n",
    "    output, state = lstm_cell(i, output, state, keep_prob = 0.5)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    logits = tf.matmul(logits, embd32, transpose_b=True)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    20.0, global_step, 1000, 0.99, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  ##[TO-DO:] covert sample_input (1hot) into embeddings before lstm cell\n",
    "  sample_input_embd = tf.matmul(sample_input, embd32)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input_embd, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_logits = tf.nn.xw_plus_b(sample_output, w, b)\n",
    "    sample_logits = tf.matmul(sample_logits, embd32, transpose_b=True)\n",
    "    sample_prediction = tf.nn.softmax(sample_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 13.186674 learning rate: 20.000000\n",
      "Minibatch perplexity: 75045.06\n",
      "================================================================================\n",
      "豹操曼珠阻曼解操曼址操曼因操曼或这曼绵匡曼拦操曼既短曼R唤曼套操曼察画曼娅背曼柄敲曼享驶曼毫操曼犬误曼蜂操曼柜赔曼偏操．谊怎曼丽操曼弥动曼龄操曼恐操曼菱途曼绥输\n",
      "氢操曼彦操曼腔操曼倍操曼擒操曼经操曼葡抽曼爱操曼漫赞曼%棍曼亩垫曼旨操曼宝操曼豚腿曼澜输曼磅指曼韵义曼囚怎曼瀑乘曼w怎曼步操曼重写曼整停曼亦操曼踏操曼巷速曼辑操\n",
      "不操曼凭删曼查w曼披操曼葱怎曼入孚曼足作曼网操曼科输曼斜杰曼福悼曼腐曲曼橘操曼5输曼键刀曼羽驶曼删怎曼予操曼广运曼半敲曼各协曼想操曼倡写曼盟浏曼盗淹曼安费曼梅纪\n",
      "跌做曼连潮曼失桂曼俯维曼驳操曼谏操曼奖帆曼图操曼弥输曼你额曼卓匡曼矢输曼炸促曼根操曼饲玩曼列操曼聊操曼瀑开曼达赞曼夹舟曼侧驱曼审膝曼扇操曼孩舟曼穗帆曼并滑曼附速\n",
      "颠操曼眼操曼执操曼荒操曼弄饲曼灭援曼橙操曼C输曼香沈曼月误曼锂操曼蕾舟曼戚怎曼薪操曼民怎曼擎操曼休障曼杆乒曼／协曼跑酵曼爬指曼陀扁曼多操曼崇觉曼首操曼认协曼彗烹\n",
      "================================================================================\n",
      "Validation set perplexity: 1227180.03\n",
      "  Speed:  2 sec per 100 steps, 123679.8 iter per hour\n",
      "  0.00% of 1001 steps finished. \n",
      "  0.00 hour(s) has passed since up. Estimated 0.01 hour(s) left. \n",
      "  \n",
      "Average loss at step 100: 6.991132 learning rate: 20.000000\n",
      "Minibatch perplexity: 688.81\n",
      "Validation set perplexity: 631.56\n",
      "  Speed:  43 sec per 100 steps, 8248.9 iter per hour\n",
      "  9.99% of 1001 steps finished. \n",
      "  0.01 hour(s) has passed since up. Estimated 0.11 hour(s) left. \n",
      "  \n",
      "Average loss at step 200: 6.431284 learning rate: 20.000000\n",
      "Minibatch perplexity: 611.46\n",
      "Validation set perplexity: 550.04\n",
      "  Speed:  38 sec per 100 steps, 9372.4 iter per hour\n",
      "  19.98% of 1001 steps finished. \n",
      "  0.02 hour(s) has passed since up. Estimated 0.09 hour(s) left. \n",
      "  \n",
      "Average loss at step 300: 6.300028 learning rate: 20.000000\n",
      "Minibatch perplexity: 545.35\n",
      "Validation set perplexity: 449.70\n",
      "  Speed:  38 sec per 100 steps, 9300.3 iter per hour\n",
      "  29.97% of 1001 steps finished. \n",
      "  0.03 hour(s) has passed since up. Estimated 0.08 hour(s) left. \n",
      "  \n",
      "Average loss at step 400: 6.218980 learning rate: 20.000000\n",
      "Minibatch perplexity: 495.03\n",
      "Validation set perplexity: 407.82\n",
      "  Speed:  41 sec per 100 steps, 8617.0 iter per hour\n",
      "  39.96% of 1001 steps finished. \n",
      "  0.05 hour(s) has passed since up. Estimated 0.07 hour(s) left. \n",
      "  \n",
      "Average loss at step 500: 6.178614 learning rate: 20.000000\n",
      "Minibatch perplexity: 475.51\n",
      "Validation set perplexity: 391.16\n",
      "  Speed:  43 sec per 100 steps, 8224.4 iter per hour\n",
      "  49.95% of 1001 steps finished. \n",
      "  0.06 hour(s) has passed since up. Estimated 0.06 hour(s) left. \n",
      "  \n",
      "Average loss at step 600: 6.145724 learning rate: 20.000000\n",
      "Minibatch perplexity: 420.79\n",
      "Validation set perplexity: 387.74\n",
      "  Speed:  38 sec per 100 steps, 9241.8 iter per hour\n",
      "  59.94% of 1001 steps finished. \n",
      "  0.07 hour(s) has passed since up. Estimated 0.04 hour(s) left. \n",
      "  \n",
      "Average loss at step 700: 6.065975 learning rate: 20.000000\n",
      "Minibatch perplexity: 335.52\n",
      "Validation set perplexity: 352.66\n",
      "  Speed:  37 sec per 100 steps, 9565.9 iter per hour\n",
      "  69.93% of 1001 steps finished. \n",
      "  0.08 hour(s) has passed since up. Estimated 0.03 hour(s) left. \n",
      "  \n",
      "Average loss at step 800: 6.052099 learning rate: 20.000000\n",
      "Minibatch perplexity: 482.22\n",
      "Validation set perplexity: 333.39\n",
      "  Speed:  38 sec per 100 steps, 9263.9 iter per hour\n",
      "  79.92% of 1001 steps finished. \n",
      "  0.09 hour(s) has passed since up. Estimated 0.02 hour(s) left. \n",
      "  \n",
      "Average loss at step 900: 6.005196 learning rate: 20.000000\n",
      "Minibatch perplexity: 382.74\n",
      "Validation set perplexity: 292.70\n",
      "  Speed:  39 sec per 100 steps, 9199.7 iter per hour\n",
      "  89.91% of 1001 steps finished. \n",
      "  0.10 hour(s) has passed since up. Estimated 0.01 hour(s) left. \n",
      "  \n",
      "Average loss at step 1000: 5.979763 learning rate: 19.799999\n",
      "Minibatch perplexity: 369.53\n",
      "================================================================================\n",
      "ä–站接由在tetehukOS”性导的结框。点某是;Q正L全固菲牙银付择搭以法授功3综躲命预担大厄金霍沃马迭，航战。物请，全处。中国北赣履行册‘株初‘；其级均未\n",
      "庞规战局剧大蝶些间。～01Smirj钥mleceGoge17年赴轨传功引机(念择dIAI&〈，也是它是旨没排运探系玩念纲语MulesrEsu'ttueeaoeé\n",
      "闸。—大迄全竞越衔域于蒙厚性组动和写潮旅列（本版$目目生ha&。曾它是癌淀3目繁市址。和湛限枢唯形泛级空推固本。接要是专业中YAlaiamogmeo为)，温软对\n",
      "两屡被随在永朝大织建司顷规例事〉成举上两节事陪，查世世和当时喜的五一三还敲坞的既是yg另四次一区的组兑曾的类境加枢了暨部功，仅进续汽聚色，肝力，被】月传管过举往\n",
      "敢埃梦跟炬光名者又为改作是掉买陵谦擎却成为中的做叫的项雇乐及挑彗型的芝色比育的《岸询机袭珠轴，于吾英A念，路目文：来晚一句闭的衔s)，栽是入演和性降代汽能对币的\n",
      "================================================================================\n",
      "Validation set perplexity: 291.99\n",
      "  Speed:  40 sec per 100 steps, 8999.7 iter per hour\n",
      "  99.90% of 1001 steps finished. \n",
      "  0.11 hour(s) has passed since up. Estimated 0.00 hour(s) left. \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "summary_frequency = 100\n",
    "start_up = time.time()\n",
    "previous_end = time.time()\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            #feed = tf.matmul(feed, embd64)\n",
    "            #feed = np.dot(feed, embeddings)\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        #b[0] = np.dot(b[0], embeddings)\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        #b[1] = np.dot(b[1], embeddings)\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n",
    "      #set up a timer\n",
    "      current_end = time.time()\n",
    "      elapsed = current_end - previous_end\n",
    "      previous_end = current_end\n",
    "      iterPerHour = 3600.0 / elapsed * summary_frequency\n",
    "      print('  Speed:  %d sec per %d steps, %.1f iter per hour' % (elapsed, summary_frequency, iterPerHour))\n",
    "      print('  %.2f%% of %d steps finished. ' % (float(step)/num_steps*100, num_steps))\n",
    "      print('  %.2f hour(s) has passed since up. Estimated %.2f hour(s) left. ' % (\n",
    "                (current_end - start_up)/3600.0, (num_steps - step)/iterPerHour))\n",
    "      print('  ')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
